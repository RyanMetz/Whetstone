
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Whetstone 0.8.9 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="welcome-to-whetstone-s-documentation">
<h1>Welcome to Whetstone’s documentation!<a class="headerlink" href="#welcome-to-whetstone-s-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
<span id="document-getting_started"></span><div class="section" id="getting-started">
<span id="id1"></span><h2>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h2>
<div class="section" id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h3>
<p>This package provides a framework for training deep spiking neural networks using keras.
Whetstone is designed to be extendable, modular, and easy-to-use.</p>
<p>The easiest way to install is:
1. Find a location where you have write permissions and would like a copy of the Whetstone package.
2. <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span></code> the repo
This will create a new sub-directory called whestone which will contain all the relevant code.
3. Run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">.</span></code> to install the package.</p>
</div>
<div class="section" id="dependencies">
<h3>Dependencies<a class="headerlink" href="#dependencies" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>Keras</li>
<li>Tensorflow or Theano (<code class="docutils literal notranslate"><span class="pre">test.py</span></code> requires Tensorflow)</li>
<li>Numpy</li>
</ul>
</div>
<div class="section" id="key-components">
<h3>Key Components<a class="headerlink" href="#key-components" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>Whetstone is designed to be as “drop-in” as possible.  With this in mind, relatively few changes are required to your standard network.</li>
<li><em>Activations</em>  Standard activation layers are static and therefore not compatible.  In <code class="docutils literal notranslate"><span class="pre">whestone.layers</span></code> you will find spiking-ready equivalents to standard or slightly modified activation functions.  For example, instead of using a standard Rectified Linear Unit, you can use <code class="docutils literal notranslate"><span class="pre">whetstone.Spiking_BRelu</span></code> which is a spiking-ready version of a Bounded Rectified Linear Unit.</li>
<li><em>Sharpener</em> You need to attach a sharpening callback.  A sharpener adjusts the activation functions over time, essentially spikifying the network.  The sharpener is responsible for determining when and by how much each layer should be sharpened.  The most basic of these is <code class="docutils literal notranslate"><span class="pre">whetstone.SimpleSharperner</span></code>.</li>
<li><em>Softmax_Decode</em> Often used for classification, this is a decoding layer that can be used for a softmax output layer.  The layer uses redudant neurons to help stabilize training.</li>
</ol>
</div>
<div class="section" id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h3>
<p>The code below can be used to train a simple densely connected spiking network for classifying mnist.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adadelta</span>
<span class="kn">from</span> <span class="nn">whetstone.layers</span> <span class="kn">import</span> <span class="n">Spiking_BRelu</span><span class="p">,</span> <span class="n">Softmax_Decode</span><span class="p">,</span> <span class="n">key_generator</span>
<span class="kn">from</span> <span class="nn">whetstone.callbacks</span> <span class="kn">import</span> <span class="n">SimpleSharpener</span>

<span class="n">numClasses</span> <span class="o">=</span> <span class="mi">10</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="n">y_train</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">numClasses</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">numClasses</span><span class="p">)</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="p">(</span><span class="mi">60000</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">))</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="p">(</span><span class="mi">10000</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">))</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">key_generator</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Spiking_BRelu</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Spiking_BRelu</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Spiking_BRelu</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Softmax_Decode</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>

<span class="n">simple</span> <span class="o">=</span> <span class="n">SimpleSharpener</span><span class="p">(</span><span class="n">start_epoch</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bottom_up</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">Adadelta</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">21</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">simple</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<span id="document-Examples"></span><div class="section" id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<p>A number of simple examples are provided in the <code class="docutils literal notranslate"><span class="pre">Examples</span></code> directory.</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">simple_mnist.py</span></code> - Light-weight demo of SimpleSharpener, Spiking_BRelu, and Softmax_Decode for a fully connected net on mnist.</li>
<li><code class="docutils literal notranslate"><span class="pre">scheduled_mnist.py</span></code> - Convolutional net trained on mnist using the ScheduledSharpener. Uses batch normalization layers during training, which are removed in the final product. Should achieve 99%+ accuracy.</li>
</ul>
</div>
<span id="document-Layers"></span><div class="section" id="module-whetstone.layers">
<span id="layers"></span><h2>Layers<a class="headerlink" href="#module-whetstone.layers" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="whetstone.layers.Softmax_Decode">
<em class="property">class </em><code class="descclassname">whetstone.layers.</code><code class="descname">Softmax_Decode</code><span class="sig-paren">(</span><em>key=None</em>, <em>size=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Softmax_Decode" title="Permalink to this definition">¶</a></dt>
<dd><p>A layer which uses a key to decode a sparse representation into a softmax.</p>
<p>Makes it easier to train spiking classifiers by allowing the use of
softmax and catagorical-crossentropy loss. Allows for encodings that are
n-hot where ‘n’ is the number of outputs assigned to each class. Allows
encodings to overlap, where a given output neuron can contribute
to the probability of more than one class.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>key: A numpy array (num_classes, input_dims) with an input_dim-sized</dt>
<dd>{0,1}-vector representative for each class.</dd>
<dt>size: A tuple (num_classes, input_dim).  If <code class="docutils literal notranslate"><span class="pre">key</span></code> is not specified, then</dt>
<dd>size must be specified.  In which case, a key will automatically be generated.</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="whetstone.layers.Softmax_Decode.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Softmax_Decode.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the layer weights.</p>
<p>Must be implemented on all layers that have weights.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Keras tensor (future input to layer)</dt>
<dd>or list/tuple of Keras tensors to reference
for weight shape computations.</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="whetstone.layers.Softmax_Decode.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Softmax_Decode.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs: Additional keyword arguments.</dd>
<dt># Returns</dt>
<dd>A tensor or list/tuple of tensors.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="whetstone.layers.Softmax_Decode.compute_output_shape">
<code class="descname">compute_output_shape</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Softmax_Decode.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Shape tuple (tuple of integers)</dt>
<dd>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
</dd>
<dt># Returns</dt>
<dd>An input shape tuple.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="whetstone.layers.Softmax_Decode.get_config">
<code class="descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Softmax_Decode.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="docutils">
<dt># Returns</dt>
<dd>Python dictionary.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="whetstone.layers.Spiking">
<em class="property">class </em><code class="descclassname">whetstone.layers.</code><code class="descname">Spiking</code><span class="sig-paren">(</span><em>sharpness=0.0</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Spiking" title="Permalink to this definition">¶</a></dt>
<dd><p>Abstract base layer for all spiking activation Layers.</p>
<p>This layer should not be instantiated, but rather inherited.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>sharpness: Float, abstract ‘sharpness’ of the activation.</dt>
<dd>Setting sharpness to 0.0 leaves the activation function unmodified.
Setting sharpness to 1.0 sets the activation function to a threshold gate.</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="whetstone.layers.Spiking.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Spiking.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the layer weights.</p>
<p>Must be implemented on all layers that have weights.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Keras tensor (future input to layer)</dt>
<dd>or list/tuple of Keras tensors to reference
for weight shape computations.</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="whetstone.layers.Spiking.get_config">
<code class="descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Spiking.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides configuration info so model can be saved and loaded.</p>
<dl class="docutils">
<dt># Returns</dt>
<dd>A dictionary of the layer’s configuration.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="whetstone.layers.Spiking.sharpen">
<code class="descname">sharpen</code><span class="sig-paren">(</span><em>amount=0.01</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Spiking.sharpen" title="Permalink to this definition">¶</a></dt>
<dd><p>Sharpens the activation function by the specified amount.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>amount: Float, the amount to sharpen.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="whetstone.layers.Spiking_BRelu">
<em class="property">class </em><code class="descclassname">whetstone.layers.</code><code class="descname">Spiking_BRelu</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Spiking_BRelu" title="Permalink to this definition">¶</a></dt>
<dd><p>A Bounded Rectified Linear Unit layer that can be sharpened to a threshold gate.</p>
<p>The sharpness value of the layer is inverted to determine the width of the
linear-region (i.e. non-binary region), which determines the slope
of the line in the linear-region such that the line intersects y = 0 and y = 1
at the current step-function borders. The line will always pass through the
point (0.5, 0.5).</p>
<dl class="method">
<dt id="whetstone.layers.Spiking_BRelu.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Spiking_BRelu.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the layer weights.</p>
<p>Must be implemented on all layers that have weights.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Keras tensor (future input to layer)</dt>
<dd>or list/tuple of Keras tensors to reference
for weight shape computations.</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="whetstone.layers.Spiking_BRelu.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Spiking_BRelu.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs: Additional keyword arguments.</dd>
<dt># Returns</dt>
<dd>A tensor or list/tuple of tensors.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="whetstone.layers.Spiking_BRelu.get_config">
<code class="descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Spiking_BRelu.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides configuration info so model can be saved and loaded.</p>
<dl class="docutils">
<dt># Returns</dt>
<dd>A dictionary of the layer’s configuration.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="whetstone.layers.Spiking_Sigmoid">
<em class="property">class </em><code class="descclassname">whetstone.layers.</code><code class="descname">Spiking_Sigmoid</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Spiking_Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>A Sigmoid layer that can be sharpened to a threshold gate.</p>
<p>The sharpness value of the layer is inverted to determine the width of the
linear-region (i.e. non-binary region). The roots of the third derivative of
the sigmoid are used to map the width to a ‘k’ value which is used to scale
the ‘x’ value in the sigmoid function, which places the knees of sigmoid
approximately at the current step-function borders.</p>
<dl class="method">
<dt id="whetstone.layers.Spiking_Sigmoid.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Spiking_Sigmoid.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the layer weights.</p>
<p>Must be implemented on all layers that have weights.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Keras tensor (future input to layer)</dt>
<dd>or list/tuple of Keras tensors to reference
for weight shape computations.</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="whetstone.layers.Spiking_Sigmoid.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Spiking_Sigmoid.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id5"><span class="problematic" id="id6">**</span></a>kwargs: Additional keyword arguments.</dd>
<dt># Returns</dt>
<dd>A tensor or list/tuple of tensors.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="whetstone.layers.Spiking_Sigmoid.get_config">
<code class="descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.Spiking_Sigmoid.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides configuration info so model can be saved and loaded.</p>
<dl class="docutils">
<dt># Returns</dt>
<dd>A dictionary of the layer’s configuration.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="attribute">
<dt id="whetstone.layers.cls">
<code class="descclassname">whetstone.layers.</code><code class="descname">cls</code><a class="headerlink" href="#whetstone.layers.cls" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#whetstone.layers.Spiking_Sigmoid" title="whetstone.layers.Spiking_Sigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">whetstone.layers.Spiking_Sigmoid</span></code></a></p>
</dd></dl>

<dl class="function">
<dt id="whetstone.layers.key_generator">
<code class="descclassname">whetstone.layers.</code><code class="descname">key_generator</code><span class="sig-paren">(</span><em>num_classes</em>, <em>width</em>, <em>sparsity=0.1</em>, <em>overlapping=True</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.layers.key_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a key to encode and decode a one-hot vector into a sparse {0,1}-vector.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><p class="first">num_classes: Integer, number of classes represented by the one-hot vector.
width: Integer, dimensionality of the expansion
sparsity: Float, approximate ratio of 1’s to 0’s in the encoded vectors.
overlapping: Boolean, if <code class="docutils literal notranslate"><span class="pre">False</span></code>, the encoded vectors are assured to</p>
<blockquote class="last">
<div>be linearly independent.</div></blockquote>
</dd>
<dt># Returns</dt>
<dd>An ndarray of size (num_classes, width)</dd>
</dl>
</dd></dl>

</div>
<span id="document-Callbacks"></span><div class="section" id="module-whetstone.callbacks">
<span id="callbacks"></span><h2>Callbacks<a class="headerlink" href="#module-whetstone.callbacks" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="whetstone.callbacks.AdaptiveSharpener">
<em class="property">class </em><code class="descclassname">whetstone.callbacks.</code><code class="descname">AdaptiveSharpener</code><span class="sig-paren">(</span><em>min_init_epochs=10</em>, <em>rate=0.25</em>, <em>cz_rate=0.126</em>, <em>critical=0.75</em>, <em>first_layer_relative_rate=1.0</em>, <em>patience=1</em>, <em>sig_increase=0.15</em>, <em>sig_decrease=0.15</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.callbacks.AdaptiveSharpener" title="Permalink to this definition">¶</a></dt>
<dd><p>Sharpens a model automatically, using training loss to control the process.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>min_init_epochs: Integer, minimum number of epochs to train before sharpening begins.
rate: Float, amount to sharpen a layer per epoch.
cz_rate: Float, rate of sharpening in Critical Zone, which is when layer sharpness &gt;= <code class="docutils literal notranslate"><span class="pre">critical</span></code>.
critical: Float, critical sharpness after which to apply cz_rate.
first_layer_relative_rate: Float, percentage of normal sharpening rate to use in first layer.
patience: Integer, how many epochs to wait for significant improvement.
sig_increase: Float, percent increase in loss considered significant.
sig_decrease: Float, percent decrease in loss considered significant.</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="whetstone.callbacks.RLSharpener">
<em class="property">class </em><code class="descclassname">whetstone.callbacks.</code><code class="descname">RLSharpener</code><span class="sig-paren">(</span><em>start_step</em>, <em>layer_duration</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.callbacks.RLSharpener" title="Permalink to this definition">¶</a></dt>
<dd><p>Experimental Sharpener for use with KerasRL.</p>
<p>Behaves like the SimpleSharpener, but based on steps instead of batches or epochs.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>start_step: Integer, step to begin sharpening.
layer_duration: Integer, number of steps over which to sharpen each layer.</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="whetstone.callbacks.ScheduledSharpener">
<em class="property">class </em><code class="descclassname">whetstone.callbacks.</code><code class="descname">ScheduledSharpener</code><span class="sig-paren">(</span><em>schedule=None</em>, <em>num_layers=None</em>, <em>start=None</em>, <em>duration=None</em>, <em>intermission=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.callbacks.ScheduledSharpener" title="Permalink to this definition">¶</a></dt>
<dd><p>Sharpens each layer according to a manually defined schedule.</p>
<p>Takes a sharpening schedule as input and gradually sharpens on each batch by
the appropriate amount, as automatically calculated, such that each layer begins
and ends sharpening as specified in the schedule. Note: The first epoch is not allowed
to perform any sharpening. This is because we need to know the number of batches per epoch.</p>
<p>If schedule isn’t passed, then num_layers, start, duration, and intermission must be supplied.
These will be used to generate a schedule (see gen_schedule method).</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first docutils">
<dt>schedule: List of tuples of the form [(start_epoch, stop_epoch), (start_epoch, stop_epoch), …]</dt>
<dd>specifying for which epoch to to begin and end sharpening for each spiking layer, where the
sharpening schedule for the ith spiking layer would be the ith tuple in the list.
Note that the first epoch is 0, not 1.</dd>
</dl>
<p class="last">num_layers: Integer, number of sharpenable layers in the model.
start: Integer, epoch number on which to begin sharpening.
duration: Integer, number of epochs over which to sharpen each layer.
intermission: Integer, number of epochs to halt sharpening between layers.</p>
</dd>
</dl>
<dl class="method">
<dt id="whetstone.callbacks.ScheduledSharpener.gen_schedule">
<code class="descname">gen_schedule</code><span class="sig-paren">(</span><em>num_layers</em>, <em>start</em>, <em>duration</em>, <em>intermission</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.callbacks.ScheduledSharpener.gen_schedule" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a sharpening schedule for use with ScheduledSharpener.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>num_layers: Integer, number of sharpenable layers in the model.
start: Integer, epoch number on which to begin sharpening.
duration: Integer, number of epochs over which to sharpen each layer.
intermission: Integer, number of epochs to halt sharpening between layers.</dd>
<dt># Returns</dt>
<dd>List of tuples of the form [(start_epoch, stop_epoch), (start_epoch, stop_epoch), …]
specifying for which epoch to to begin and end sharpening for each spiking layer, where the
sharpening schedule for the ith spiking layer would be the ith tuple in the list.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="whetstone.callbacks.Sharpener">
<em class="property">class </em><code class="descclassname">whetstone.callbacks.</code><code class="descname">Sharpener</code><span class="sig-paren">(</span><em>bottom_up=True</em>, <em>verbose=False</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.callbacks.Sharpener" title="Permalink to this definition">¶</a></dt>
<dd><p>Absract base class used for different sharpening callbacks.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first docutils">
<dt>bottom_up: Boolean, if <code class="docutils literal notranslate"><span class="pre">True</span></code>, sharpens one layer at a time,</dt>
<dd>sequentially, starting with the first. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, sharpens all layers uniformly.</dd>
</dl>
<p class="last">verbose: Boolean, if <code class="docutils literal notranslate"><span class="pre">True</span></code>, prints status updates during training.</p>
</dd>
</dl>
<dl class="method">
<dt id="whetstone.callbacks.Sharpener.set_layer_sharpness">
<code class="descname">set_layer_sharpness</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.callbacks.Sharpener.set_layer_sharpness" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the sharpness values of all spiking layers.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>values: A list of sharpness values (between 0.0 and 1.0 inclusive) for each</dt>
<dd>spiking layer in the same order as their indices.</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="whetstone.callbacks.Sharpener.set_model_sharpness">
<code class="descname">set_model_sharpness</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.callbacks.Sharpener.set_model_sharpness" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>Sets the sharpness of the whole model either in a bottom_up or uniform fashion depending on the</dt>
<dd>value of the bottom_up instance variable.</dd>
<dt># Arguments</dt>
<dd>value: Float, value between 0.0 and 1.0 inclusive that specifies the sharpness of the model.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="whetstone.callbacks.SimpleSharpener">
<em class="property">class </em><code class="descclassname">whetstone.callbacks.</code><code class="descname">SimpleSharpener</code><span class="sig-paren">(</span><em>start_epoch</em>, <em>steps=4</em>, <em>epochs=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.callbacks.SimpleSharpener" title="Permalink to this definition">¶</a></dt>
<dd><p>Basic sharpener that sharpens each layer in a set number of batches.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>start_epoch: Integer, epoch on which to begin sharpening.
steps: Integer, number of steps by which each layer should be fully sharpened.
epochs: Boolean, if <code class="docutils literal notranslate"><span class="pre">True</span></code>, step on each epoch.  Otherwise, step on each batch.</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="whetstone.callbacks.WhetstoneLogger">
<em class="property">class </em><code class="descclassname">whetstone.callbacks.</code><code class="descname">WhetstoneLogger</code><span class="sig-paren">(</span><em>logdir</em>, <em>sharpener=None</em>, <em>test_set=None</em>, <em>log_weights=False</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.callbacks.WhetstoneLogger" title="Permalink to this definition">¶</a></dt>
<dd><p>Keras callback that handles logging (not a type of beer).</p>
<blockquote>
<div>Automatically creates a separate subfolder for each epoch.</div></blockquote>
<dl class="docutils">
<dt># Arguments</dt>
<dd><p class="first">logdir: Directory in which to log results.
sharpener: Reference to callback of type <code class="docutils literal notranslate"><span class="pre">Sharpener</span></code>.</p>
<blockquote>
<div>If passed, metadata from the sharpener will be recorded.</div></blockquote>
<dl class="last docutils">
<dt>test_set: Test set tuple in form (x_test, y_test).</dt>
<dd>If passed, test set accuracy will be evaluated on current and
fully-sharpened versions of the net at the end of each epoch.</dd>
<dt>log_weights: Boolean, if <code class="docutils literal notranslate"><span class="pre">True</span></code>, logs weights of the entire net at the end of</dt>
<dd>each epoch.</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
<span id="document-LayerUtils"></span><div class="section" id="module-whetstone.utils.layer_utils">
<span id="layerutils"></span><h2>LayerUtils<a class="headerlink" href="#module-whetstone.utils.layer_utils" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="whetstone.utils.layer_utils.decode_from_key">
<code class="descclassname">whetstone.utils.layer_utils.</code><code class="descname">decode_from_key</code><span class="sig-paren">(</span><em>key</em>, <em>input_vec</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.utils.layer_utils.decode_from_key" title="Permalink to this definition">¶</a></dt>
<dd><p>Decodes a vector using the specified key.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>key: Key used for decoding (ndarray)
input_vec: Vector of size key.shape[1] to be decoded.</dd>
<dt># Returns</dt>
<dd>Decoded one-hot vector.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="whetstone.utils.layer_utils.encode_with_key">
<code class="descclassname">whetstone.utils.layer_utils.</code><code class="descname">encode_with_key</code><span class="sig-paren">(</span><em>key</em>, <em>input_vec</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.utils.layer_utils.encode_with_key" title="Permalink to this definition">¶</a></dt>
<dd><p>Encodes a vector using the specified key.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>key: Key used for encoding (ndarray)
input_vec: Vector of size key.shape[0] to be encoded.</dd>
<dt># Returns</dt>
<dd>Encoded {0,1}-vector.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="whetstone.utils.layer_utils.get_spiking_layer_indices">
<code class="descclassname">whetstone.utils.layer_utils.</code><code class="descname">get_spiking_layer_indices</code><span class="sig-paren">(</span><em>model</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.utils.layer_utils.get_spiking_layer_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns indices of layers that can be sharpened.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>model: Keras model with one or more Spiking layers.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="whetstone.utils.layer_utils.load_model">
<code class="descclassname">whetstone.utils.layer_utils.</code><code class="descname">load_model</code><span class="sig-paren">(</span><em>filepath</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.utils.layer_utils.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a keras model that can contain custom Whetstone layers.</p>
<p>Loads and returns the Keras/Whetstone model from a .h5 file at <code class="docutils literal notranslate"><span class="pre">filepath</span></code>, handling custom layer
deserialization for you.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>filepath: Path to Keras/Whetstone model which should be a .h5 file produced by model.save(filepath).</dd>
<dt># Returns</dt>
<dd>A keras Model.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="whetstone.utils.layer_utils.set_layer_sharpness">
<code class="descclassname">whetstone.utils.layer_utils.</code><code class="descname">set_layer_sharpness</code><span class="sig-paren">(</span><em>model</em>, <em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.utils.layer_utils.set_layer_sharpness" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the sharpness values of all spiking layers.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><p class="first">model: Keras model with one or more Spiking layers.
values: A list of sharpness values (between 0.0 and 1.0 inclusive) for each</p>
<blockquote class="last">
<div>spiking layer in the same order as their indices.</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="whetstone.utils.layer_utils.set_model_sharpness">
<code class="descclassname">whetstone.utils.layer_utils.</code><code class="descname">set_model_sharpness</code><span class="sig-paren">(</span><em>model</em>, <em>value</em>, <em>bottom_up</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.utils.layer_utils.set_model_sharpness" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the sharpness of the whole model.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">bottom_up</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> sharpens in bottom-up order, otherwise sharpens uniformly.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>model: Keras model with one or more Spiking layers.
value: Float, between 0.0 and 1.0 inclusive that specifies the sharpness of the model.
bottom_up: Boolean, if <code class="docutils literal notranslate"><span class="pre">True</span></code> then sharpens in bottom-up order, else uniform.</dd>
</dl>
</dd></dl>

</div>
<span id="document-ExportUtils"></span><div class="section" id="module-whetstone.utils.export_utils">
<span id="exportutils"></span><h2>ExportUtils<a class="headerlink" href="#module-whetstone.utils.export_utils" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="whetstone.utils.export_utils.copy_remove_batchnorm">
<code class="descclassname">whetstone.utils.export_utils.</code><code class="descname">copy_remove_batchnorm</code><span class="sig-paren">(</span><em>keras_sequential_model</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.utils.export_utils.copy_remove_batchnorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a functionally equivalent copy of a net with the BatchNormalization layers removed.</p>
<p>Given a keras Sequential model, returns an equivalent Sequential model without batchnorm layers.
Assumes you only use batch normalization directly after a layer with activation=None.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>keras_sequential_model: The Sequential model to be copied.</dd>
<dt># Returns</dt>
<dd>new_model: A copy of keras_sequential_model with the batch normalization layers removed.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="whetstone.utils.export_utils.merge_batchnorm">
<code class="descclassname">whetstone.utils.export_utils.</code><code class="descname">merge_batchnorm</code><span class="sig-paren">(</span><em>keras_preactivation_layer</em>, <em>keras_batchnorm_layer</em><span class="sig-paren">)</span><a class="headerlink" href="#whetstone.utils.export_utils.merge_batchnorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Merges the parameters of a batch normalization layer into the layer before it.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>keras_preactivation_layer: Layer directly preceding the batchnorm layer.
keras_batchnorm_layer: The batch normalization layer to be merged into the preactivation layer.</dd>
<dt># Returns</dt>
<dd>new_layer, (new_weights, new_biases) : Where new_layer is a keras layer of the same configuration
as keras_preactivation_layer, and (new_weights, new_biases) are the updated weights and biases
for the new_layer to be used with new_layer.set_weights(((new_weights, new_biases))) after the
new model is built.</dd>
</dl>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="whetstone">
<h1>Whetstone<a class="headerlink" href="#whetstone" title="Permalink to this headline">¶</a></h1>
<p>This package provides a framework for training deep spiking neural networks using keras.
Whetstone is designed to be extendable, modular, and easy-to-use.</p>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<p>Before an initial release, you must clone the git repo and install the package manually.</p>
<p>The easiest way to this is:
1. Find a location where you have write permissions and would like a copy of the Whetstone package.
2. <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span> <span class="pre">https://github.com/SNL-NERL/Whetstone</span></code>
This will create a new sub-directory called whestone which will contain all the relevant code.
3. Run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">.</span></code> to install the package.</p>
</div>
<div class="section" id="dependencies">
<h2>Dependencies<a class="headerlink" href="#dependencies" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Keras 2.1.5</li>
<li>Tensorflow 1.3.0</li>
<li>Numpy</li>
</ul>
</div>
<div class="section" id="example">
<h2>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<p>The code below can be used to train a simple densely connected spiking network for classifying mnist.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">..</span> <span class="n">code</span><span class="o">-</span><span class="n">block</span><span class="p">::</span> <span class="n">python</span>
</pre></div>
</div>
<blockquote>
<div><p>import numpy as np
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.utils import to_categorical
from keras.layers import Dense
from whetstone.layers import Spiking_BRelu, Softmax_Decode
from whetstone.utils import key_generator</p>
<p>numClasses = 10
(x_train, y_train),(x_test, y_test) = mnist.load_data()</p>
<p>y_train = to_categorical(y_train, numClasses)
y_test = to_categorical(y_test, numClasses)</p>
<p>x_train = np.reshape(x_train, (60000,28*28))
x_test = np.reshape(x_test, (10000,28*28))</p>
<p>key = key_generator(10,100)</p>
<p>model = Sequential()
model.add(Dense(256, input_shape=(28*28,)))
model.add(Spiking_BRelu())
model.add(Dense(64))
model.add(Spiking_BRelu())
model.add(Dense(10))
model.add(Spiking_BRelu())
model.add(Softmax_Decode(key))</p>
<p>simple = SimpleSharpener(5,epochs=True)</p>
<p>model.compile(loss=’categorical_crossentropy’, optimizer=’adam’)
model.fit(x_train,y_train,epochs=15,callbacks=[simple],metrics=[‘accuracy’])</p>
<p>print(model.evaluate(x_test,y_test))</p>
</div></blockquote>
<p>For more information, see Getting Started.</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h3><a href="index.html#document-index">Table Of Contents</a></h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-getting_started">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-Examples">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-Layers">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-Callbacks">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-LayerUtils">LayerUtils</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-ExportUtils">ExportUtils</a></li>
</ul>

        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018 National Technology & Engineering Solutions of Sandia, LLC (NTESS).
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.7.8</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.11</a>
      
    </div>

    

    
  </body>
</html>